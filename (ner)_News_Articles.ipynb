{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvRdtgObmJ8xLG7xQzPgAF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/assermahmoud99/internship-tasks/blob/main/(ner)_News_Articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrsttYYgfUSa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca424739-b633-43f7-cd07-35316ea9f76f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Downloading the dataset\n",
        "\n",
        "Here, we download the English CoNLL03 dataset directly from KaggleHub. This dataset is widely used for NER tasks and contains newswire text with named entity annotations. We prepare three splits: train, validation, and test.\n",
        "\n"
      ],
      "metadata": {
        "id": "oLeekqcWQY9k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjBmZjw8RaFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb00d17-8764-4838-d26b-0f535859dd44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/alaakhaled/conll003-englishversion?dataset_version_number=1&file_name=train.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 650k/650k [00:00<00:00, 92.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting zip of train.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'conll003-englishversion' dataset.\n",
            "Using Colab cache for faster access to the 'conll003-englishversion' dataset.\n",
            "Train file path: /root/.cache/kagglehub/datasets/alaakhaled/conll003-englishversion/versions/1/train.txt\n",
            "Validation file path: /kaggle/input/conll003-englishversion/valid.txt\n",
            "Test file path: /kaggle/input/conll003-englishversion/test.txt\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download the CoNLL2003 dataset\n",
        "path_train = kagglehub.dataset_download(\"alaakhaled/conll003-englishversion\", path=\"train.txt\")\n",
        "path_valid = kagglehub.dataset_download(\"alaakhaled/conll003-englishversion\", path=\"valid.txt\")\n",
        "path_test  = kagglehub.dataset_download(\"alaakhaled/conll003-englishversion\", path=\"test.txt\")\n",
        "\n",
        "print(\"Train file path:\", path_train)\n",
        "print(\"Validation file path:\", path_valid)\n",
        "print(\"Test file path:\", path_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preprocessing into sentences\n",
        "\n",
        "The CoNLL03 files are stored in token-per-line format, with blank lines separating sentences. This function rebuilds those into proper sentences by joining tokens until a blank line. Here we load the first 200 sentences for testing and print a preview.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VlEb8n8dqFxK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hzyX_KskaL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb6619c2-33f4-4334-9200-9fb309b280b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "['-DOCSTART-', 'EU rejects German call to boycott British lamb .', 'Peter Blackburn', 'BRUSSELS 1996-08-22', 'The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep .']\n"
          ]
        }
      ],
      "source": [
        "# Step 1. Install & import\n",
        "!pip install spacy kagglehub\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "from spacy.matcher import Matcher\n",
        "from spacy import displacy\n",
        "\n",
        "\n",
        "def load_sentences(filepath, limit=200):\n",
        "    sentences = []\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        block = []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if block:\n",
        "                    sentences.append(\" \".join(block))\n",
        "                    block = []\n",
        "            else:\n",
        "                word = line.split()[0]\n",
        "                block.append(word)\n",
        "            if len(sentences) >= limit:\n",
        "                break\n",
        "    return sentences\n",
        "\n",
        "train_sentences = load_sentences(path_train, limit=200)\n",
        "print(train_sentences[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Loading spaCy models\n",
        "We load two different spaCy models:\n",
        "\n",
        "\n",
        "*   en_core_web_sm: a small, faster model.\n",
        "\n",
        "*   en_core_web_md: a larger, more accurate model.\n",
        "\n",
        "\n",
        "\n",
        "This allows us to compare model-based NER performance between lightweight and more advanced spaCy models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TOWq0yuhs1TW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_sm = spacy.load(\"en_core_web_sm\")\n",
        "nlp_md = spacy.load(\"en_core_web_md\")\n"
      ],
      "metadata": {
        "id": "EbY2CdXEkGsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model-based NER (small model)\n",
        "\n",
        "Here we apply spaCy’s pretrained small model (sm) to extract named entities from all sentences. The results are stored in a DataFrame with entity text and label (e.g. PERSON, ORG, LOC). We then:\n",
        "\n",
        "\n",
        "\n",
        "*   Show the first 20 detected entities.\n",
        "\n",
        "*   Count how many entities of each type appear.\n",
        "*   Display the most frequent entity mentions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This demonstrates the model-based NER approach.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yg-feLLs3Yzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_entities = []\n",
        "\n",
        "for sent in train_sentences:\n",
        "    doc = nlp_sm(sent)\n",
        "    for ent in doc.ents:\n",
        "        all_entities.append((ent.text, ent.label_))\n",
        "\n",
        "# Save to DataFrame\n",
        "entities_df = pd.DataFrame(all_entities, columns=[\"entity\", \"label\"])\n",
        "print(entities_df.head(20))\n",
        "\n",
        "print(\"Entity type counts:\\n\")\n",
        "print(entities_df[\"label\"].value_counts())\n",
        "\n",
        "print(\"\\nMost common entities:\\n\")\n",
        "print(entities_df[\"entity\"].value_counts().head(20))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymHjfGb6xBaZ",
        "outputId": "51fa2fac-bc40-46bb-92f3-e09ed027a92b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     entity   label\n",
            "0                        EU     ORG\n",
            "1                    German    NORP\n",
            "2                   British    NORP\n",
            "3           Peter Blackburn  PERSON\n",
            "4                  BRUSSELS     GPE\n",
            "5                1996-08-22    DATE\n",
            "6   The European Commission     ORG\n",
            "7                  Thursday    DATE\n",
            "8                    German    NORP\n",
            "9                   British    NORP\n",
            "10                  Germany     GPE\n",
            "11    the European Union 's     ORG\n",
            "12         Werner Zwingmann  PERSON\n",
            "13                Wednesday    DATE\n",
            "14                  Britain     GPE\n",
            "15               Commission     ORG\n",
            "16     Nikolaus van der Pas  PERSON\n",
            "17       the European Union     ORG\n",
            "18               last month    DATE\n",
            "19                  EU Farm     ORG\n",
            "Entity type counts:\n",
            "\n",
            "label\n",
            "GPE         139\n",
            "DATE         91\n",
            "NORP         88\n",
            "PERSON       74\n",
            "ORG          73\n",
            "CARDINAL     47\n",
            "MONEY        11\n",
            "PERCENT      11\n",
            "LOC           5\n",
            "ORDINAL       4\n",
            "EVENT         2\n",
            "TIME          2\n",
            "QUANTITY      1\n",
            "LANGUAGE      1\n",
            "PRODUCT       1\n",
            "FAC           1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Most common entities:\n",
            "\n",
            "entity\n",
            "1996-08-22       15\n",
            "Thursday         15\n",
            "Israel           14\n",
            "Israeli          12\n",
            "Arafat            9\n",
            "Syria             9\n",
            "Taiwan            7\n",
            "Iranian           7\n",
            "Kurdish           7\n",
            "German            7\n",
            "Polish            6\n",
            "Wednesday         6\n",
            "British           6\n",
            "Iraq              6\n",
            "China             6\n",
            "Iraqi             6\n",
            "Iran              6\n",
            "the West Bank     6\n",
            "Taleban           6\n",
            "Palestinian       6\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Rule-based NER (small model)\n",
        "\n",
        "This section uses spaCy’s Matcher to implement a rule-based NER system:\n",
        "\n",
        "\n",
        "*   PROPER_NOUN: sequences of capitalized words.\n",
        "*   MONEY: numbers followed by words like “dollars”, “million”.\n",
        "*   DATE: number followed by a capitalized token (like “1996 August”).\n",
        "\n",
        "We then apply these patterns to the dataset and count how many matches each rule produced. This shows how custom rules can detect entities without training a model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2foBbAEcQU-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "matcher = Matcher(nlp_sm.vocab)\n",
        "pattern = [{\"IS_TITLE\": True, \"OP\": \"+\"}]\n",
        "matcher.add(\"PROPER_NOUN\", [[{\"IS_TITLE\": True, \"OP\": \"+\"}]])\n",
        "matcher.add(\"MONEY\", [[{\"LIKE_NUM\": True}, {\"LOWER\": {\"IN\": [\"dollars\",\"usd\",\"$\",\"million\",\"billion\"]}}]])\n",
        "matcher.add(\"DATE\", [[{\"LIKE_NUM\": True}, {\"IS_TITLE\": True}]])\n",
        "\n",
        "rule_entities = []\n",
        "for sent in train_sentences:\n",
        "    doc = nlp_sm(sent)\n",
        "    matches = matcher(doc)\n",
        "    for match_id, start, end in matches:\n",
        "        span = doc[start:end]\n",
        "        rule_entities.append((span.text, nlp_sm.vocab.strings[match_id]))\n",
        "\n",
        "rule_df = pd.DataFrame(rule_entities, columns=[\"entity\", \"rule_label\"])\n",
        "print(rule_df['rule_label'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlrfnbmmPsYx",
        "outputId": "333764ae-a25d-4749-f459-b61676527792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rule_label\n",
            "PROPER_NOUN    828\n",
            "DATE            11\n",
            "MONEY            2\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Highlighting extracted entities\n",
        "\n",
        "\n",
        "\n",
        "Here we take a real sentence from the dataset and display its named entities both in text (entity → label) and visually using displaCy. This satisfies the requirement to highlight and categorize extracted entities.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xUTINBfIAp9f"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "dfa75e4e",
        "outputId": "eec00e11-f0c3-4a32-834d-0d55ec91dcbb"
      },
      "source": [
        "\n",
        "sample_text = train_sentences[10]\n",
        "doc = nlp_sm(sample_text)\n",
        "\n",
        "print(\"=== Highlighted Entities ===\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text:15} --> {ent.label_}\")\n",
        "\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Highlighted Entities ===\n",
            "Fischler        --> PERSON\n",
            "EU              --> ORG\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">But \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Fischler\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " agreed to review his proposal after the \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    EU\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " 's standing veterinary committee , mational animal health officials , questioned if such action was justified as there was only a slight risk to human health .</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Model-based NER (medium model)\n",
        "\n",
        "\n",
        "We repeat the model-based extraction using en_core_web_md. This allows direct comparison with the small model, since the medium model usually detects more entities and labels them more accurately."
      ],
      "metadata": {
        "id": "nGrM10ciCSck"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e974c82",
        "outputId": "6137c604-cb7f-48c6-a703-05b0f82908a1"
      },
      "source": [
        "all_entities_md = []\n",
        "\n",
        "for sent in train_sentences:\n",
        "    doc = nlp_md(sent)\n",
        "    for ent in doc.ents:\n",
        "        all_entities_md.append((ent.text, ent.label_))\n",
        "\n",
        "entities_df_md = pd.DataFrame(all_entities_md, columns=[\"entity\", \"label\"])\n",
        "print(entities_df_md.head(20))\n",
        "print(\"Entity type counts:\\n\")\n",
        "print(entities_df_md[\"label\"].value_counts())\n",
        "\n",
        "print(\"\\nMost common entities:\\n\")\n",
        "print(entities_df_md[\"entity\"].value_counts().head(20))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     entity   label\n",
            "0                -DOCSTART-     ORG\n",
            "1                        EU     ORG\n",
            "2                    German    NORP\n",
            "3                   British    NORP\n",
            "4           Peter Blackburn  PERSON\n",
            "5       BRUSSELS 1996-08-22     ORG\n",
            "6   The European Commission     ORG\n",
            "7                  Thursday    DATE\n",
            "8                    German    NORP\n",
            "9                   British    NORP\n",
            "10                  Germany     GPE\n",
            "11    the European Union 's     ORG\n",
            "12         Werner Zwingmann  PERSON\n",
            "13                Wednesday    DATE\n",
            "14                  Britain     GPE\n",
            "15               Commission     ORG\n",
            "16     Nikolaus van der Pas  PERSON\n",
            "17       the European Union     ORG\n",
            "18               last month    DATE\n",
            "19                  EU Farm     ORG\n",
            "Entity type counts:\n",
            "\n",
            "label\n",
            "GPE         149\n",
            "NORP         93\n",
            "DATE         92\n",
            "ORG          85\n",
            "PERSON       79\n",
            "CARDINAL     46\n",
            "PERCENT      11\n",
            "MONEY        10\n",
            "LOC           4\n",
            "ORDINAL       4\n",
            "PRODUCT       3\n",
            "EVENT         2\n",
            "TIME          2\n",
            "QUANTITY      1\n",
            "FAC           1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Most common entities:\n",
            "\n",
            "entity\n",
            "-DOCSTART-       16\n",
            "Israel           15\n",
            "Thursday         15\n",
            "Israeli          12\n",
            "1996-08-22       11\n",
            "Arafat            9\n",
            "Syria             9\n",
            "Taleban           8\n",
            "Iranian           7\n",
            "Taiwan            7\n",
            "German            7\n",
            "Kurdish           7\n",
            "China             6\n",
            "Wednesday         6\n",
            "Iraqi             6\n",
            "the West Bank     6\n",
            "Iran              6\n",
            "Iraq              6\n",
            "Polish            6\n",
            "British           6\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Rule-based NER (medium model)\n",
        "\n",
        "We also apply the rule-based approach on the medium model. This demonstrates that rule-based NER can be applied independently of which spaCy pipeline we use.\n"
      ],
      "metadata": {
        "id": "VeGowvUcCd1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matcher_md = Matcher(nlp_md.vocab)\n",
        "pattern = [{\"IS_TITLE\": True, \"OP\": \"+\"}]\n",
        "matcher_md.add(\"PROPER_NOUN\", [[{\"IS_TITLE\": True, \"OP\": \"+\"}]])\n",
        "matcher_md.add(\"MONEY\", [[{\"LIKE_NUM\": True}, {\"LOWER\": {\"IN\": [\"dollars\",\"usd\",\"$\",\"million\",\"billion\"]}}]])\n",
        "matcher_md.add(\"DATE\", [[{\"LIKE_NUM\": True}, {\"IS_TITLE\": True}]])\n",
        "\n",
        "rule_entities_md = []\n",
        "for sent in train_sentences:\n",
        "    doc_md = nlp_md(sent)\n",
        "    matches = matcher(doc_md)\n",
        "    for match_id, start, end in matches:\n",
        "        span_md = doc_md[start:end]\n",
        "        rule_entities_md.append((span_md.text, nlp_md.vocab.strings[match_id]))\n",
        "\n",
        "rule_df_md = pd.DataFrame(rule_entities_md, columns=[\"entity\", \"rule_label\"])\n",
        "print(rule_df['rule_label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWVGAJusue6j",
        "outputId": "3eedb20b-17b0-43fa-90f4-37cca591e40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rule_label\n",
            "PROPER_NOUN    828\n",
            "DATE            11\n",
            "MONEY            2\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}