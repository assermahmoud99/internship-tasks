{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPt9S3VC2XAH0MxA+WeyUKr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/assermahmoud99/internship-tasks/blob/main/News_topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "R59iFVkINR_I",
        "outputId": "907d4f17-c121-4715-cc5f-7aede3e67030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "9e48705458204812853d272f92d1a2a8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. Downloading the Dataset from Kaggle\n",
        "\n",
        "\n",
        "*   Here I used KaggleHub to automatically download the BBC news dataset.\n",
        "*   The dataset has one single csv file which contains the entire data>\n",
        "*   I stored it in path loading it later with pandas.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o_rY-jwLpMOr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjBmZjw8RaFT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce4a5bc-94f5-467f-8521-262b84ecfab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'bbc-news' dataset.\n",
            "Path to dataset files: /kaggle/input/bbc-news/bbc_news.csv\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"gpreda/bbc-news\",path='bbc_news.csv')\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importing Libraries & Loading the Dataset\n",
        "\n",
        "Here, I imported the required libraries that we are going to use and assigned the number of rows to 15k.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VlEb8n8dqFxK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_hzyX_KskaL5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "\n",
        "df = pd.read_csv(path,nrows=15000) #extracting the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocessing the News Articles\n",
        "\n",
        "Here we prepare the dataset for topic modeling. We combine each article’s title and description into a single field called content. Then, using spaCy, we clean the text by:\n",
        "\n",
        "\n",
        "*   Tokenizing the sentences into words.\n",
        "\n",
        "*   Lowercasing everything.\n",
        "*   Removing stopwords and non-alphabetic tokens.\n",
        "\n",
        "\n",
        "*   Lemmatizing words (reducing them to their base form).\n",
        "\n",
        "\n",
        "The result is stored in a new column token, which contains a clean list of tokens for each article.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TOWq0yuhs1TW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df[\"content\"] = df[\"title\"].fillna(\"\") + \" \" + df[\"description\"].fillna(\"\")\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "tokens = []\n",
        "def clean_text(news):\n",
        "  doc = nlp(news)\n",
        "  tokens = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
        "  return tokens\n",
        "df['token'] = df['content'].astype(str).apply(clean_text)\n",
        "df['token'].head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "EbY2CdXEkGsY",
        "outputId": "2ebb9fd1-ed76-4c34-8900-d7bbc3419645"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [ukraine, angry, zelensky, vow, punish, russia...\n",
              "1    [war, ukraine, take, cover, town, attack, jere...\n",
              "2    [ukraine, war, catastrophic, global, food, wor...\n",
              "3    [manchester, arena, bombing, saffie, roussos, ...\n",
              "4    [ukraine, conflict, oil, price, soar, high, le...\n",
              "5    [ukraine, war, pm, hold, talk, world, leader, ...\n",
              "6    [ukraine, war, uk, grant, ukrainian, refugee, ...\n",
              "7    [tiktok, limit, service, netflix, pull, russia...\n",
              "8    [covid, fourth, jab, scotland, vulnerable, tes...\n",
              "9    [protest, russia, thousand, detain, people, ho...\n",
              "Name: token, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[ukraine, angry, zelensky, vow, punish, russia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[war, ukraine, take, cover, town, attack, jere...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[ukraine, war, catastrophic, global, food, wor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[manchester, arena, bombing, saffie, roussos, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[ukraine, conflict, oil, price, soar, high, le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[ukraine, war, pm, hold, talk, world, leader, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[ukraine, war, uk, grant, ukrainian, refugee, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[tiktok, limit, service, netflix, pull, russia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[covid, fourth, jab, scotland, vulnerable, tes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[protest, russia, thousand, detain, people, ho...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Building the Dictionary, Corpus, and Training LDA\n",
        "\n",
        "Next, we convert the cleaned tokens into a format that the LDA model can understand.\n",
        "\n",
        "\n",
        "*   We build a Dictionary (word → unique ID mapping).\n",
        "\n",
        "*   We filter out very rare words (appear in <5 documents) and very common words (appear in >50% of documents).\n",
        "\n",
        "*   We create a Bag-of-Words corpus where each document is represented as a list of word IDs and counts.\n",
        "*   Finally, we train a Latent Dirichlet Allocation (LDA) model with 10 topics. The model tries to uncover hidden themes across the articles.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h5xo1Vve0nS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = Dictionary(df['token'])\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
        "corpus = [dictionary.doc2bow(tokens) for tokens in df['token']]\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=10,\n",
        "    random_state=42,\n",
        "    passes=10,\n",
        "    per_word_topics=True\n",
        ")"
      ],
      "metadata": {
        "id": "O9z0VtU0mtX0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Displaying the Most Significant Words per Topic\n",
        "\n",
        "After training, we extract the top 5 words per topic from the LDA model. These words are the most representative terms for each latent topic discovered. We store them in a pandas DataFrame for better readability. This output lets us interpret the meaning of each topic (e.g., politics, sports, health, economy) based on its most frequent and significant words."
      ],
      "metadata": {
        "id": "Yg-feLLs3Yzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topics_data = []\n",
        "for idx, topic in lda_model.show_topics(formatted=False, num_words=5):\n",
        "    words = [word for word, prob in topic]\n",
        "    topics_data.append({\"Topic\": idx, \"Top Words\": \", \".join(words)})\n",
        "\n",
        "topics_df = pd.DataFrame(topics_data)\n",
        "print(topics_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymHjfGb6xBaZ",
        "outputId": "8c6b80fe-5da9-4aec-9cfc-467701a14aba"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Topic                                    Top Words\n",
            "0      0  league, champions, arrest, liverpool, score\n",
            "1      1              say, king, ireland, truss, open\n",
            "2      2           ukraine, war, russia, russian, say\n",
            "3      3        queen, covid, earthquake, people, day\n",
            "4      4    manchester, city, united, league, premier\n",
            "5      5              uk, sunak, government, new, say\n",
            "6      6              strike, cost, pay, rise, living\n",
            "7      7               uk, say, papers, lead, johnson\n",
            "8      8                year, police, die, bbc, woman\n",
            "9      9              world, cup, england, win, final\n"
          ]
        }
      ]
    }
  ]
}